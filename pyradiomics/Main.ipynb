{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import graphviz\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "import sklearn\n",
    "from sklearn.linear_model import SGDRegressor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T17:40:27.271807600Z",
     "start_time": "2023-11-02T17:40:26.974687700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lung Cancer Classification using the LIDC-IDRI Dataset\n",
    "\n",
    "*Elias Huber, Filipe Carvalho, Diederik Dekker, Jose Guedes*\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Table of Contents\n",
    "1. [Introduction](#introduction)\n",
    "2. [Pylidc](#pylidc)\n",
    "3. [Pyradiomics](#pyradiomics)\n",
    "4. [Data Analysis](#data-analysis)\n",
    "5. [Results](#results)\n",
    "6. [Conclusion](#conclusion)\n",
    "7. [References](#references)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. <a id='introduction'>Introduction</a>\n",
    "\n",
    "The objective of the Laboratory of Artificial Intelligence and Data Science (Lab AI & DC) course\n",
    "is to provide students with software development methodologies, AI and DC projects, teamwork and\n",
    "communication through the implementation of projects designed for this purpose. Students should\n",
    "apply the knowledge obtained from the courses from previous years and research methodologies to\n",
    "solve the problem. In this first project, the students will use images as input data, namely Computerized Tomography (CT) data, from the human torso, for Lung Cancer classification. "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. <a id='pylidc'>Pylidc</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'pylidc_csv.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[66], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#Create the dataframe from de CSV file\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m df\u001B[38;5;241m=\u001B[39m\u001B[43mpd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mpylidc_csv.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Bureaublad\\studie tn\\Minor vakken Porto\\IA CAD\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001B[0m, in \u001B[0;36mread_csv\u001B[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001B[0m\n\u001B[0;32m    935\u001B[0m kwds_defaults \u001B[38;5;241m=\u001B[39m _refine_defaults_read(\n\u001B[0;32m    936\u001B[0m     dialect,\n\u001B[0;32m    937\u001B[0m     delimiter,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    944\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[0;32m    945\u001B[0m )\n\u001B[0;32m    946\u001B[0m kwds\u001B[38;5;241m.\u001B[39mupdate(kwds_defaults)\n\u001B[1;32m--> 948\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_read\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Bureaublad\\studie tn\\Minor vakken Porto\\IA CAD\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001B[0m, in \u001B[0;36m_read\u001B[1;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[0;32m    608\u001B[0m _validate_names(kwds\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnames\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m))\n\u001B[0;32m    610\u001B[0m \u001B[38;5;66;03m# Create the parser.\u001B[39;00m\n\u001B[1;32m--> 611\u001B[0m parser \u001B[38;5;241m=\u001B[39m \u001B[43mTextFileReader\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilepath_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    613\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m chunksize \u001B[38;5;129;01mor\u001B[39;00m iterator:\n\u001B[0;32m    614\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m parser\n",
      "File \u001B[1;32m~\\OneDrive\\Bureaublad\\studie tn\\Minor vakken Porto\\IA CAD\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1448\u001B[0m, in \u001B[0;36mTextFileReader.__init__\u001B[1;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[0;32m   1445\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptions[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m kwds[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhas_index_names\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m   1447\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles: IOHandles \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m-> 1448\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_make_engine\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mengine\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Bureaublad\\studie tn\\Minor vakken Porto\\IA CAD\\venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1705\u001B[0m, in \u001B[0;36mTextFileReader._make_engine\u001B[1;34m(self, f, engine)\u001B[0m\n\u001B[0;32m   1703\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m mode:\n\u001B[0;32m   1704\u001B[0m         mode \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m-> 1705\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;241m=\u001B[39m \u001B[43mget_handle\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1706\u001B[0m \u001B[43m    \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1707\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1708\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1709\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompression\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcompression\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1710\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmemory_map\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmemory_map\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1711\u001B[0m \u001B[43m    \u001B[49m\u001B[43mis_text\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mis_text\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1712\u001B[0m \u001B[43m    \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mencoding_errors\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstrict\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1713\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstorage_options\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1714\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1715\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1716\u001B[0m f \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[1;32m~\\OneDrive\\Bureaublad\\studie tn\\Minor vakken Porto\\IA CAD\\venv\\Lib\\site-packages\\pandas\\io\\common.py:863\u001B[0m, in \u001B[0;36mget_handle\u001B[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[0;32m    858\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(handle, \u001B[38;5;28mstr\u001B[39m):\n\u001B[0;32m    859\u001B[0m     \u001B[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001B[39;00m\n\u001B[0;32m    860\u001B[0m     \u001B[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001B[39;00m\n\u001B[0;32m    861\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mencoding \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m ioargs\u001B[38;5;241m.\u001B[39mmode:\n\u001B[0;32m    862\u001B[0m         \u001B[38;5;66;03m# Encoding\u001B[39;00m\n\u001B[1;32m--> 863\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[0;32m    864\u001B[0m             handle,\n\u001B[0;32m    865\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[0;32m    866\u001B[0m             encoding\u001B[38;5;241m=\u001B[39mioargs\u001B[38;5;241m.\u001B[39mencoding,\n\u001B[0;32m    867\u001B[0m             errors\u001B[38;5;241m=\u001B[39merrors,\n\u001B[0;32m    868\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    869\u001B[0m         )\n\u001B[0;32m    870\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    871\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m    872\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'pylidc_csv.csv'"
     ]
    }
   ],
   "source": [
    "#Create the dataframe from de CSV file\n",
    "df=pd.read_csv('pylidc/pylidc_csv.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T16:44:40.975385900Z",
     "start_time": "2023-11-02T16:44:37.385770300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#In this first section we present some information on the data\n",
    "df.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.655361400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#The number of nods for each patient is distrubuted according to the following frequency table \n",
    "sum=0\n",
    "print(\"Number of nods \", \"Relative frequency\")\n",
    "for j in range(1,10):\n",
    "    sum+=df['N nods'].value_counts()[j]/len(df)\n",
    "    print(\"       \",j, \"         \" ,(df['N nods'].value_counts()[j]/len(df)).round(2))\n",
    "print(\"   Total:          \",sum.round(2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.658491200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Number of patients')\n",
    "print(df[\"Patient_id\"].nunique())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.664251700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#To better understand the way to classify a nod to be malignant or not we present the related variables "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.670008700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sns.countplot(x='Malignancy_min',data=df))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.676480200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sns.histplot(x='Malignancy_mean',data=df,bins=[0,0.5,1.5,2.5,3.5,4.5,5.5]))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.681994900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sns.countplot(x='Malignancy_max',data=df))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.687840200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sns.countplot(x='Malignancy_n4',data=df))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.693132800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sns.countplot(x='Malignancy_n5',data=df))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.697790100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Noticing that the distribution of the mean value and the max value is similar we explored it a litle further \n",
    "df['dif']=df['Malignancy_max']-df['Malignancy_mean']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.702213Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sns.histplot(x='dif',data=df))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.708372900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Despite the similarities between the mean and the max value we decided to create a joint criterion for the \n",
    "# malignancy classification. Therefore a node is considered malignant if there is at least one classification of 5 or\n",
    "# if the mean value over the classifications is at least 3.5\n",
    "\n",
    "df[\"Malignancy\"]=np.where((df['Malignancy_mean']>= 3.5)|(df['Malignancy_max']==5), True, False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.714899200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[{'Malignancy_mean','Malignancy_max','Malignancy'}].head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.719934500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Identify the features used to predict the classification of the malignancy of the node\n",
    "# X - vector of valiables used to predict\n",
    "# y - target variable\n",
    "\n",
    "X=df[['N nods','Spiculation_Min', 'Spiculation_Med', 'Spiculation_Max',\n",
    "       'Internal_Structure', 'Calcification', 'Sphericity', 'Margin_min',\n",
    "       'Margin_mean', 'Margin_max', 'Lobulation_min', 'Lobulation_mean',\n",
    "       'Lobulation_max', 'Texture']]\n",
    "y=df['Malignancy']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.723956100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Devide the dataset in train (80%) and test (20%) subsets\n",
    "# Define a random state do fix this division\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2,random_state=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.726951300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Construction of a decision tree classifier with the stop criterion of \n",
    "# at least 0.001 of minimum impurity decrease to avoind overfitting\n",
    "\n",
    "dt = tree.DecisionTreeClassifier(min_impurity_decrease=0.001)\n",
    "dt = dt.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.730457900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print of the more significant separations with percentage of the subset (train) and proportion for each separation\n",
    "\n",
    "dt_data = export_graphviz(dt,feature_names=X_train.columns,filled=True,max_depth=2,impurity=False,proportion=True)\n",
    "graph = graphviz.Source(dt_data)\n",
    "display(graph)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.734470800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test of the decision tree obtained using the test subset\n",
    "y_predict=dt.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "confusion_matrix(y_test, y_predict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.737513Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Accuracy:\")\n",
    "accuracy_score(y_test, y_predict)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.740090400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Random Forest construction with trees similar to the ones obtained previously\n",
    "# for different values of hyperparameter - number of features (max_features) \n",
    "\n",
    "rf = RandomForestClassifier(max_features=1,min_impurity_decrease=0.001)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "#print(\"Number of featrures: \", 1 ,\" Accuracy: \", accuracy)\n",
    "best=1\n",
    "best_ac=accuracy\n",
    "x_rf=[1]\n",
    "y_rf=[accuracy]\n",
    "    \n",
    "for i in range(2,10):\n",
    "    rf = RandomForestClassifier(max_features=i,min_impurity_decrease=0.001,n_estimators=100)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    #print(\"Number of featrures: \", i ,\" Accuracy: \", accuracy)\n",
    "    x_rf.append(i)\n",
    "    y_rf.append(accuracy)\n",
    "    if(accuracy>best_ac):\n",
    "        best=i\n",
    "        best_ac=accuracy\n",
    "\n",
    "#print(\"Number of featrures: \", best ,\" Accuracy: \", best_ac)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.741992300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the hyperparameter and identify the best value for the accuracy\n",
    "\n",
    "s = pd.Series(y_rf,x_rf)\n",
    "plt.title('Accuracy for different number of features')\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "s.plot.line()  \n",
    "print(\"Best number of features: \", best ,\" with accuracy=\", best_ac)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.743994600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print of the more significant separations with percentage of the subset (train) and proportion for each separation\n",
    "# for the two first trees\n",
    "\n",
    "rf = RandomForestClassifier(max_features=best,min_impurity_decrease=0.001)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "for i in range(2):\n",
    "    tree = rf.estimators_[i]\n",
    "    dot_data = export_graphviz(tree,feature_names=X_train.columns,filled=True,max_depth=2,impurity=False,proportion=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    display(graph)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.746998900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#KNN\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Knn construction for different values of hyperparameter - number of neighbors (n_neighbors) \n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train) \n",
    "y_pred = knn.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "#print(\"Number of neighbors: \",\"1\", \"Accuracy: \", accuracy)\n",
    "best=1\n",
    "best_ac=accuracy\n",
    "x_knn=[1]\n",
    "y_knn=[accuracy]\n",
    "\n",
    "for i in range(2,10):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train) \n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    #print(\"Number of neighbors: \",i, \"Accuracy: \", accuracy)\n",
    "    x_knn.append(i)\n",
    "    y_knn.append(accuracy)\n",
    "    if(accuracy>best_ac):\n",
    "        best=i\n",
    "        best_ac=accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.748442700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the hyperparameter and identify the best value for the accuracy\n",
    "\n",
    "s = pd.Series(y_knn,x_knn)\n",
    "plt.title('Accuracy for different number of neighbors')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "s.plot.line()  \n",
    "print(\"Best number of neighbors: \", best ,\" with accuracy=\", best_ac)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T16:44:38.751881300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. <a id='pyradiomics'>Pyradiomics</a\n",
    " "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Feature extraction\n",
    "Code for feature extraction using pyradiomics and dependencies: dcm2niix and SimpleITK. We built a docker image for running the pyradiomics-dcm.py script that is available on the pyradiomics github under labs. The script was still experimental and outdated so we had to make adjustments in the script and dockerfile to make it run. If you want to install the docker container yourself we can provide the working dockerfile. \n",
    "The Feature extraction uses the pylidc library and to channel directories and read out the corresponding segmentation and icom files for each patient. These then get passed to the pyradiomics docker container with the experimental script for evaluation. \n",
    "The result is a vector with 1574 features. These are put together with metadata and saved inside a csv file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Function that calculates features using the pyradiomics-dcm.py script from the pyradiomics GIT\n",
    "def get_features():\n",
    "    start_time = time.time()\n",
    "    # put path of dataset here\n",
    "    parent_dir = r\"C:\\Users\\Diederik\\OneDrive\\Bureaublad\\studie tn\\Minor vakken Porto\\IA CAD\\Images+seg\\manifest-1698154951594\"\n",
    "    patient_dicom_path_mounted = r\"/data/Images+seg/manifest-1698154951594/LIDC-IDRI\"\n",
    "    # get path of LIDC-IDRI directionary\n",
    "    data_dir = os.path.join(parent_dir, \"LIDC-IDRI\")\n",
    "    # give directory where docker saves files\n",
    "    docker_save_dir = r\"C:\\Users\\Diederik\\OneDrive\\Bureaublad\\studie tn\\Minor vakken Porto\\IA CAD\"\n",
    "    # give the hash of the pyradiomnics docker\n",
    "    docker_hash = r\"d95ce08239e3182d8631d3492a5e4a32096d28285c3d2f10dd570d7e6d06fd01\"\n",
    "    # path to the features dict\n",
    "    features_dict = r\"/data/test/featuresDict_IBSIv7.tsv\"\n",
    "    # pyradiomics save folder\n",
    "    pyradiomics_midsave_path = r\"/data/pyradiomics converter test\"\n",
    "    # temporal dir\n",
    "    temp_dir = r\"C:\\Users\\Diederik\\OneDrive\\Bureaublad\\studie tn\\Minor vakken Porto\\IA CAD\\test\\temp file\"\n",
    "    parameter_file = r\"C:\\Users\\Diederik\\OneDrive\\Bureaublad\\studie tn\\Minor vakken Porto\\IA CAD\\test\\Pyradiomics_Params_test.yaml\"\n",
    "    data = pd.read_csv(r\"C:\\Users\\Diederik\\OneDrive\\Bureaublad\\studie tn\\Minor vakken Porto\\IA CAD\\test\\features.csv\")\n",
    "    df = pd.read_excel(r\"C:\\Users\\Diederik\\OneDrive\\Bureaublad\\studie tn\\Minor vakken Porto\\IA CAD\\test\\nodule_counts_by_patient.xlsx\")\n",
    "    df = df.drop(df.columns[[4, 5]], axis=1)\n",
    "    df.columns = ['Patient_ID', 'Total_Nodule_Count', 'NodG3','NodL3']\n",
    "    dataframe = pd.DataFrame(\n",
    "        columns=['Patient_ID', 'Nodule', ' Annotation', 'Subtlety', 'InternalStructure', 'Calcification', 'Sphericity',\n",
    "                 'Margin', 'Lobulation', 'Spiculation', 'Texture', 'Malignancy'])\n",
    "    backup = 0\n",
    "    iteration_counter = 1\n",
    "    for p_id in df['Patient_ID']:\n",
    "\n",
    "        print(\"Patient \" + str(p_id) + \"Processing\")\n",
    "        if os.path.isdir(os.path.join(data_dir, str(p_id))) == False:\n",
    "            print(\"Patient \" + str(p_id) + \" not found\")\n",
    "            continue  # if the patient folder doesn't exist, skip it\n",
    "\n",
    "        scan = pl.query(pl.Scan).filter(pl.Scan.patient_id == p_id).first()\n",
    "        nods = scan.cluster_annotations()\n",
    "\n",
    "        # path to the patient folder\n",
    "        patient_dir = os.path.join(data_dir, str(p_id))\n",
    "        # path to dicom ct-scans of patient\n",
    "        patient_dicom_path = scan.get_path_to_dicom_files()\n",
    "\n",
    "        patient_folders = os.path.join(patient_dir, os.listdir(patient_dir)[0])\n",
    "        if len(os.listdir(patient_folders))==1:\n",
    "            shutil.rmtree(os.path.join(patient_dir, os.listdir(patient_dir)[0]))\n",
    "            patient_folders = os.path.join(patient_dir, os.listdir(patient_dir)[0])\n",
    "            print(\"wrong folder removed!\")\n",
    "\n",
    "        # listing all the folders from a patient\n",
    "        patient_seg_folders = os.listdir(patient_folders)\n",
    "        for folder in patient_seg_folders:\n",
    "            if \"evaluations\" in folder:\n",
    "                shutil.rmtree(os.path.join(patient_folders,folder))\n",
    "        patient_seg_folders = os.listdir(patient_folders)\n",
    "        # saving the dicom images folder path\n",
    "        # get all seg folders for nodules later\n",
    "\n",
    "\n",
    "        #if scan is None: # if the scan is not available we continue\n",
    "        #    continue\n",
    "\n",
    "        nod = 1\n",
    "        annot = 0\n",
    "        for nodule in nods:\n",
    "            for ann in nodule:\n",
    "                if annot >= len(patient_seg_folders):\n",
    "                    continue\n",
    "                backup += 1 #backupcounter\n",
    "\n",
    "                iteration_counter += 1\n",
    "                seg_folder = os.path.join(patient_folders, patient_seg_folders[annot+1])\n",
    "\n",
    "                # check how many files are in the segmentation folder\n",
    "                seg_files = os.listdir(seg_folder)\n",
    "                if len(seg_files) <= 0:\n",
    "                    # add a row with NaN values to the dataframe\n",
    "                    data.loc[len(data)] = [None] * len(data.columns)\n",
    "                # iterating over each segmentation file\n",
    "                for file in os.listdir(seg_folder):\n",
    "                    if file.endswith(\".dcm\"):\n",
    "                        seg_file_path = os.path.join(seg_folder, file)\n",
    "                        print(\"docker run -v \\\"\" + docker_save_dir + \":/data\\\" \" + docker_hash + \" --input-image-dir \\\"/data/\" + os.path.relpath(patient_dicom_path, docker_save_dir).replace(chr(92),\"/\") +  \"\\\" --input-seg-file \\\"/data/\" + os.path.relpath(seg_file_path, docker_save_dir).replace(chr(92),\"/\") + \"\\\" --output-dir \\\"\" + pyradiomics_midsave_path + \"\\\" --volume-reconstructor dcm2niix --features-dict \\\"/data/\" + os.path.relpath(features_dict, docker_save_dir).replace(chr(92),\"/\") + \"\\\" --temp-dir \\\"/data/\" + os.path.relpath(temp_dir, docker_save_dir).replace(chr(92),\"/\") + \"\\\" --correct-mask --parameters \\\"/data/\" + os.path.relpath(parameter_file, docker_save_dir).replace(chr(92),\"/\") + \"\\\"\")\n",
    "                        os.system(\"docker run -v \\\"\" + docker_save_dir + \":/data\\\" \" + docker_hash + \" --input-image-dir \\\"/data/\" + os.path.relpath(patient_dicom_path, docker_save_dir).replace(chr(92),\"/\") +  \"\\\" --input-seg-file \\\"/data/\" + os.path.relpath(seg_file_path, docker_save_dir).replace(chr(92),\"/\") + \"\\\" --output-dir \\\"\" + pyradiomics_midsave_path + \"\\\" --volume-reconstructor dcm2niix --features-dict \\\"/data/\" + os.path.relpath(features_dict, docker_save_dir).replace(chr(92),\"/\") + \"\\\" --temp-dir \\\"/data/\" + os.path.relpath(temp_dir, docker_save_dir).replace(chr(92),\"/\") + \"\\\" --correct-mask --parameters \\\"/data/\" + os.path.relpath(parameter_file, docker_save_dir).replace(chr(92),\"/\") + \"\\\"\")\n",
    "\n",
    "                        try:\n",
    "                            testdata = pd.read_csv(r\"C:\\Users\\Diederik\\OneDrive\\Bureaublad\\studie tn\\Minor vakken Porto\\IA CAD\\test\\temp file\\Features\\1.csv\")\n",
    "                            #print(testdata)\n",
    "                            # append data to features.csv\n",
    "                            #print(data.info())\n",
    "                            #data = data.append(testdata)\n",
    "                            data = pd.concat([data, testdata], ignore_index=True)\n",
    "                            #print(data)\n",
    "\n",
    "\n",
    "                        except:\n",
    "                            # append a row with NaN values to the dataframe\n",
    "                            data.loc[len(data)] = [None] * len(data.columns)\n",
    "                            thisdir = os.getcwd()\n",
    "                            os.chdir(parent_dir)\n",
    "                            # write to a log file the patient name, the seg folder name and the file name\n",
    "                            log = open(\"log.txt\", \"a\")\n",
    "                            log.write(\"Failed to extract features from: \" + os.getcwd() + \"\\n\")\n",
    "                            log.write(\"SEG File: \" + file + \"\\n\\n\")\n",
    "                            os.chdir(thisdir)\n",
    "                            continue\n",
    "                        # delete temp folder\n",
    "                        os.system(\"rmdir /s /q temp\")\n",
    "                        print(\"\\n\\n\")\n",
    "                    else:\n",
    "                        # also append a row with NaN values to the dataframe\n",
    "                        data.loc[len(data)] = [None] * len(data.columns)\n",
    "\n",
    "                # create feature vector\n",
    "                feature = list(ann.feature_vals())\n",
    "                feature.insert(0, annot)\n",
    "                feature.insert(0, nod)\n",
    "                feature.insert(0, p_id)\n",
    "                dataframe.loc[len(dataframe)] = feature\n",
    "\n",
    "                thisdir = os.getcwd()\n",
    "\n",
    "                # create a backup of the dataframes every 10 iterations (every 5 annotations)\n",
    "                if backup % 10 == 0:\n",
    "                    current_time = time.time()\n",
    "                    runtime = (current_time - start_time)/60\n",
    "                    print('Iteration: ' + str(iteration_counter) + '-----Backup create------------time:' + str(runtime))\n",
    "                    os.chdir(r\"C:\\Users\\Diederik\\OneDrive\\Bureaublad\\studie tn\\Minor vakken Porto\\IA CAD\\test\\Backups\")\n",
    "\n",
    "                    data.to_csv(\"pyradiomicsBackup.csv\", index=False)\n",
    "                    dataframe.to_csv(\"pylidcBackup.csv\", index=False)\n",
    "\n",
    "                    df1 = pd.read_csv(\"pylidcBackup.csv\")\n",
    "                    df2 = pd.read_csv(\"pyradiomicsBackup.csv\")\n",
    "\n",
    "                    df3 = pd.concat([df1, df2], axis=1)\n",
    "                    df3.to_csv(\"total_data_obliterationBackup.csv\", index=False)\n",
    "                os.chdir(thisdir)\n",
    "\n",
    "                annot += 1\n",
    "            nod += 1\n",
    "    os.chdir(parent_dir)\n",
    "\n",
    "    dataframe.to_csv(\"pylidc.csv\", index=False)\n",
    "    data.to_csv(\"pyradiomics.csv\", index=False)\n",
    "\n",
    "    df1 = pd.read_csv(\"pylidc.csv\")\n",
    "    df2 = pd.read_csv(\"pyradiomics.csv\")\n",
    "\n",
    "    # concatenate the columns from both dataframes\n",
    "    df3 = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "    df3.to_csv(\"total_data_obliteration.csv\", index=False)\n",
    "\n",
    "#get_features()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Feature Selection\n",
    "Since it is inefficient and vulnerable to overfitting if we would use all the GLCM features, we use a statistical p-test (ANOVA) to select the most significant features for the learning model.  \n",
    "The following code was used for feature selection."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sklearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[92], line 80\u001B[0m\n\u001B[0;32m     77\u001B[0m y \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCategory\u001B[39m\u001B[38;5;124m\"\u001B[39m]  \u001B[38;5;66;03m# y is the \"Category\" column\u001B[39;00m\n\u001B[0;32m     78\u001B[0m y_alt \u001B[38;5;241m=\u001B[39m df[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMalignancy\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m---> 80\u001B[0m X_new, f_statistic, p_values \u001B[38;5;241m=\u001B[39m \u001B[43mf_selection_Percentile\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     81\u001B[0m feature_select_plot(f_statistic,p_values)\n\u001B[0;32m     82\u001B[0m X_new_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(X_new)\n",
      "Cell \u001B[1;32mIn[92], line 48\u001B[0m, in \u001B[0;36mf_selection_Percentile\u001B[1;34m(X, y, p)\u001B[0m\n\u001B[0;32m     46\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mf_selection_Percentile\u001B[39m(X,y,p\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m):\n\u001B[0;32m     47\u001B[0m     X_new \u001B[38;5;241m=\u001B[39m SelectPercentile(f_classif, percentile\u001B[38;5;241m=\u001B[39mp)\u001B[38;5;241m.\u001B[39mfit_transform(X,y)\n\u001B[1;32m---> 48\u001B[0m     f_statistic, p_values \u001B[38;5;241m=\u001B[39m \u001B[43msklearn\u001B[49m\u001B[38;5;241m.\u001B[39mfeature_selection\u001B[38;5;241m.\u001B[39mf_classif(X,y)\n\u001B[0;32m     49\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m X_new, f_statistic, p_values\n",
      "\u001B[1;31mNameError\u001B[0m: name 'sklearn' is not defined"
     ]
    }
   ],
   "source": [
    "#-loading data-> backup of 02/11/2023\n",
    "#pylidc data\n",
    "pl_data = pd.read_csv(\"Unsorted/Data files/CSV DATA FILES/backup_02_11_2023_afternoon/pylidcBackup.csv\")\n",
    "\n",
    "#pyradiomics data\n",
    "pr_data = pd.read_csv(\"Unsorted/Data files/CSV DATA FILES/backup_02_11_2023_afternoon/pyradiomicsBackup.csv\")\n",
    "\n",
    "#Combining pl and pr data\n",
    "df = pd.concat([pl_data, pr_data], axis=1)\n",
    "df.to_csv('Python scrips/ML pyradiomics/total_data.csv')\n",
    "\n",
    "def cleaning_data(df):\n",
    "    df = df.select_dtypes(include=[int, float])\n",
    "    df = df.drop(df[df['Malignancy'] == 3].index)\n",
    "    # Remove rows with NaN values in the \"Malignancy\" column\n",
    "    df.dropna(subset=[\"Malignancy\"], inplace=True)\n",
    "    threshold = df.shape[1] - 10  # 10 or more NaN values to be dropped\n",
    "    df = df.dropna(thresh=threshold)\n",
    "    return df\n",
    "def normalize_data(df):\n",
    "    normalized_data = (df-df.min())/(df.max()-df.min())\n",
    "    return normalized_data\n",
    "\n",
    "def create_category_column(df):\n",
    "    # Create a new \"Category\" column based on the \"Malignancy\" values\n",
    "    df[\"Category\"] = df[\"Malignancy\"].apply(lambda x: 1 if x in [4, 5] else 0)\n",
    "    # drop all categories which are not numerical\n",
    "    df = df.select_dtypes(include=[int, float])\n",
    "    constant_columns = [col for col in df.columns if df[col].nunique() == 1]\n",
    "    df = df.drop(columns=constant_columns)\n",
    "    return df\n",
    "\n",
    "def get_malignancy_column_dtype(df):\n",
    "    # Check if the \"Malignancy\" column exists in the DataFrame\n",
    "    if \"Malignancy\" in df.columns:\n",
    "        malignancy_dtype = df[\"Malignancy\"].dtype\n",
    "        return malignancy_dtype\n",
    "    else:\n",
    "        return \"Column 'Malignancy' not found in the DataFrame.\"\n",
    "\n",
    "def f_selection_KBest(X,y,k=100):\n",
    "    X_new = SelectKBest(f_classif, k=k).fit_transform(X, y)\n",
    "    f_statistic, p_values = f_classif(X, y)\n",
    "    return X_new, f_statistic, p_values\n",
    "\n",
    "def f_selection_Percentile(X,y,p=10):\n",
    "    X_new = SelectPercentile(f_classif, percentile=p).fit_transform(X,y)\n",
    "    f_statistic, p_values = sklearn.feature_selection.f_classif(X,y)\n",
    "    return X_new, f_statistic, p_values\n",
    "\n",
    "def feature_select_plot(f_stat,p_val):\n",
    "    # Create a DataFrame to store and sort the F-statistic and p-values\n",
    "    results_df = pd.DataFrame({'F-Statistic': f_stat, 'p-Value': p_val})\n",
    "    results_df = results_df.sort_values(by='p-Value', ascending=True)\n",
    "    x_values = np.arange(0, len(f_stat), 1)\n",
    "\n",
    "    # Plot the sorted F-statistic and p-values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.barh(x_values, results_df['F-Statistic'], color='b')\n",
    "    plt.xlabel('F-Statistic')\n",
    "    plt.title('F-Statistic')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(x_values, -np.log10(results_df['p-Value']), color='g')\n",
    "    plt.xlabel('-log10(p-Value)')\n",
    "    plt.title('p-Values (log scale)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "df = cleaning_data(df)\n",
    "get_malignancy_column_dtype(df)\n",
    "df = create_category_column(df)\n",
    "df_norm = normalize_data(df)\n",
    "\n",
    "# Creating the ML Dataset\n",
    "X = df_norm.drop(columns=[\"Category\",\"Malignancy\"])  # X contains all columns except \"Category\"\n",
    "y = df[\"Category\"]  # y is the \"Category\" column\n",
    "y_alt = df[\"Malignancy\"]\n",
    "\n",
    "X_new, f_statistic, p_values = f_selection_Percentile(X,y)\n",
    "feature_select_plot(f_statistic,p_values)\n",
    "X_new_df = pd.DataFrame(X_new)\n",
    "\n",
    "X_new_df.to_csv(\"Python scrips/ML pyradiomics/X.csv\", index=False)\n",
    "y.to_csv(\"Python scrips/ML pyradiomics/y.csv\", index=False)\n",
    "y_alt.to_csv(\"Python scrips/ML pyradiomics/y_alt.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T17:42:07.425847900Z",
     "start_time": "2023-11-02T17:42:02.363558100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model 1: Linear Support Vector Classifier\n",
    "\n",
    "*text*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"Python scrips/ML pyradiomics/X.csv\")\n",
    "Y = pd.read_csv(\"Python scrips/ML pyradiomics/y.csv\")\n",
    "Y = Y.values.ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T17:26:28.879585900Z",
     "start_time": "2023-11-02T17:26:28.665096500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [
    "#The model\n",
    "def LSVC(X,Y,epochs, test_size):\n",
    "    e = np.arange(0, epochs)\n",
    "    acc_array = np.array([])\n",
    "    for i in e:\n",
    "        # divide training and test data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size)\n",
    "        model = LinearSVC()\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "        acc_array = np.append(acc_array, accuracy)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "    #calculate average accurcy\n",
    "    mean_acc = np.mean(acc_array)\n",
    "\n",
    "    #make a plot\n",
    "    plt.plot(e, acc_array)\n",
    "    plt.rcParams['font.size'] = '11'\n",
    "    #plt.figure(figsize=(8, 6), dpi=1000)\n",
    "    plt.title('Accuracy per epoch ' + \"Dataset size: \" + str(len(X)))\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.ylim(-0.05, 1.05)\n",
    "    plt.axhline(mean_acc, color='red', linestyle='dashed', label='Mean accuracy')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "#LSVC(X,Y,100, 0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T17:28:44.035749900Z",
     "start_time": "2023-11-02T17:28:43.676780600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model 2: Stochastic Gradient Descent Regressor\n",
    "\n",
    "*text*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "X = pd.read_csv(\"Python scrips/ML pyradiomics/X.csv\")\n",
    "Y = pd.read_csv(\"Python scrips/ML pyradiomics/y_alt.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T17:24:26.000778400Z",
     "start_time": "2023-11-02T17:24:25.795365300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#The Model\n",
    "def SGDR(X,Y,epochs, test_size):\n",
    "    e = np.arange(0, epochs)\n",
    "    acc_array = np.array([])\n",
    "    for i in e:\n",
    "        # divide training and test data\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size)\n",
    "        model.fit(X_train, y_train)\n",
    "        predictions = model.predict(X_test)\n",
    "        predictions_rounded = [round(num) for num in predictions]\n",
    "        accuracy = accuracy_score(y_test, predictions_rounded)\n",
    "        acc_array = np.append(acc_array, accuracy)\n",
    "        print(\"Accuracy:\", accuracy)\n",
    "\n",
    "    #make a plot\n",
    "    plt.plot(e, acc_array)\n",
    "    plt.rcParams['font.size'] = '11'\n",
    "    #plt.figure(figsize=(8, 6), dpi=1000)\n",
    "    plt.title('Accuracy per epoch ' + \"Dataset size: \" + str(len(X)))\n",
    "    plt.legend()\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('accuracy')\n",
    "    mean_acc = np.mean(acc_array)\n",
    "    plt.axhline(mean_acc, color='red', linestyle='dashed', label='Mean accuracy')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "#SGDR(X,Y,100, 0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T17:23:18.645925300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. <a id='data-analysis'>Data Analysis & Results</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5. <a id='discussion'>Discussion</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. <a id='conclusions'>Conclusion</a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. <a id='references'>References</a>\n",
    "\n",
    "https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LAB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
