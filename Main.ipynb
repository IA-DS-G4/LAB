{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#LAB AI PROJECT 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import graphviz"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Create the dataframe from de CSV file\n",
    "df=pd.read_csv('pylidc_csv.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#In this first section we present some information on the data\n",
    "df.head(5)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#The number of nods for each patient is distrubuted according to the following frequency table \n",
    "sum=0\n",
    "print(\"Number of nods \", \"Relative frequency\")\n",
    "for j in range(1,10):\n",
    "    sum+=df['N nods'].value_counts()[j]/len(df)\n",
    "    print(\"       \",j, \"         \" ,(df['N nods'].value_counts()[j]/len(df)).round(2))\n",
    "print(\"   Total:          \",sum.round(2))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Number of patients')\n",
    "print(df[\"Patient_id\"].nunique())"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#To better understand the way to classify a nod to be malignant or not we present the related variables "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sns.countplot(x='Malignancy_min',data=df))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sns.histplot(x='Malignancy_mean',data=df,bins=[0,0.5,1.5,2.5,3.5,4.5,5.5]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sns.countplot(x='Malignancy_max',data=df))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sns.countplot(x='Malignancy_n4',data=df))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sns.countplot(x='Malignancy_n5',data=df))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Noticing that the distribution of the mean value and the max value is similar we explored it a litle further \n",
    "df['dif']=df['Malignancy_max']-df['Malignancy_mean']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(sns.histplot(x='dif',data=df))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Despite the similarities between the mean and the max value we decided to create a joint criterion for the \n",
    "# malignancy classification. Therefore a node is considered malignant if there is at least one classification of 5 or\n",
    "# if the mean value over the classifications is at least 3.5\n",
    "\n",
    "df[\"Malignancy\"]=np.where((df['Malignancy_mean']>= 3.5)|(df['Malignancy_max']==5), True, False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df[{'Malignancy_mean','Malignancy_max','Malignancy'}].head(5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Identify the features used to predict the classification of the malignancy of the node\n",
    "# X - vector of valiables used to predict\n",
    "# y - target variable\n",
    "\n",
    "X=df[['N nods','Spiculation_Min', 'Spiculation_Med', 'Spiculation_Max',\n",
    "       'Internal_Structure', 'Calcification', 'Sphericity', 'Margin_min',\n",
    "       'Margin_mean', 'Margin_max', 'Lobulation_min', 'Lobulation_mean',\n",
    "       'Lobulation_max', 'Texture']]\n",
    "y=df['Malignancy']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Devide the dataset in train (80%) and test (20%) subsets\n",
    "# Define a random state do fix this division\n",
    "\n",
    "X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2,random_state=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Construction of a decision tree classifier with the stop criterion of \n",
    "# at least 0.001 of minimum impurity decrease to avoind overfitting\n",
    "\n",
    "dt = tree.DecisionTreeClassifier(min_impurity_decrease=0.001)\n",
    "dt = dt.fit(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print of the more significant separations with percentage of the subset (train) and proportion for each separation\n",
    "\n",
    "dt_data = export_graphviz(dt,feature_names=X_train.columns,filled=True,max_depth=2,impurity=False,proportion=True)\n",
    "graph = graphviz.Source(dt_data)\n",
    "display(graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test of the decision tree obtained using the test subset\n",
    "y_predict=dt.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "confusion_matrix(y_test, y_predict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"Accuracy:\")\n",
    "accuracy_score(y_test, y_predict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Random Forest construction with trees similar to the ones obtained previously\n",
    "# for different values of hyperparameter - number of features (max_features) \n",
    "\n",
    "rf = RandomForestClassifier(max_features=1,min_impurity_decrease=0.001)\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "#print(\"Number of featrures: \", 1 ,\" Accuracy: \", accuracy)\n",
    "best=1\n",
    "best_ac=accuracy\n",
    "x_rf=[1]\n",
    "y_rf=[accuracy]\n",
    "    \n",
    "for i in range(2,10):\n",
    "    rf = RandomForestClassifier(max_features=i,min_impurity_decrease=0.001,n_estimators=100)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    #print(\"Number of featrures: \", i ,\" Accuracy: \", accuracy)\n",
    "    x_rf.append(i)\n",
    "    y_rf.append(accuracy)\n",
    "    if(accuracy>best_ac):\n",
    "        best=i\n",
    "        best_ac=accuracy\n",
    "\n",
    "#print(\"Number of featrures: \", best ,\" Accuracy: \", best_ac)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the hyperparameter and identify the best value for the accuracy\n",
    "\n",
    "s = pd.Series(y_rf,x_rf)\n",
    "plt.title('Accuracy for different number of features')\n",
    "plt.xlabel('Number of features')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "s.plot.line()  \n",
    "print(\"Best number of features: \", best ,\" with accuracy=\", best_ac)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print of the more significant separations with percentage of the subset (train) and proportion for each separation\n",
    "# for the two first trees\n",
    "\n",
    "rf = RandomForestClassifier(max_features=best,min_impurity_decrease=0.001)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "for i in range(2):\n",
    "    tree = rf.estimators_[i]\n",
    "    dot_data = export_graphviz(tree,feature_names=X_train.columns,filled=True,max_depth=2,impurity=False,proportion=True)\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    display(graph)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#KNN\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Knn construction for different values of hyperparameter - number of neighbors (n_neighbors) \n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train) \n",
    "y_pred = knn.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "#print(\"Number of neighbors: \",\"1\", \"Accuracy: \", accuracy)\n",
    "best=1\n",
    "best_ac=accuracy\n",
    "x_knn=[1]\n",
    "y_knn=[accuracy]\n",
    "\n",
    "for i in range(2,10):\n",
    "    knn = KNeighborsClassifier(n_neighbors=i)\n",
    "    knn.fit(X_train, y_train) \n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    #print(\"Number of neighbors: \",i, \"Accuracy: \", accuracy)\n",
    "    x_knn.append(i)\n",
    "    y_knn.append(accuracy)\n",
    "    if(accuracy>best_ac):\n",
    "        best=i\n",
    "        best_ac=accuracy"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Evaluate the hyperparameter and identify the best value for the accuracy\n",
    "\n",
    "s = pd.Series(y_knn,x_knn)\n",
    "plt.title('Accuracy for different number of neighbors')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "s.plot.line()  \n",
    "print(\"Best number of neighbors: \", best ,\" with accuracy=\", best_ac)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "fjjfkgjfkdjlhdhljgfklh"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pylidc as pl\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "import time"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Code for feature extraction using pyradiomics and dependencies: dcm2niix and SimpleITK. We built docker container to run the code inside:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 155\u001B[0m\n\u001B[0;32m    151\u001B[0m     df3 \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat([df1, df2], axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    153\u001B[0m     df3\u001B[38;5;241m.\u001B[39mto_csv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtotal_data_obliteration.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m--> 155\u001B[0m \u001B[43mget_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[1], line 3\u001B[0m, in \u001B[0;36mget_features\u001B[1;34m()\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_features\u001B[39m():\n\u001B[1;32m----> 3\u001B[0m     start_time \u001B[38;5;241m=\u001B[39m \u001B[43mtime\u001B[49m\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;66;03m# put path of dataset here\u001B[39;00m\n\u001B[0;32m      5\u001B[0m     parent_dir \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mC:\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mUsers\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mDiederik\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mOneDrive\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mBureaublad\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mstudie tn\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mMinor vakken Porto\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mIA CAD\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mImages+seg\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mmanifest-1698154951594\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "#Function that calculates features using the pyradiomics-dcm.py script from the oyradiomics GIT\n",
    "def get_features():\n",
    "    start_time = time.time()\n",
    "    # put path of dataset here\n",
    "    parent_dir = r\"C:\\Users\\Diederik\\OneDrive\\Bureaublad\\studie tn\\Minor vakken Porto\\IA CAD\\Images+seg\\manifest-1698154951594\"\n",
    "    patient_dicom_path_mounted = r\"/data/Images+seg/manifest-1698154951594/LIDC-IDRI\"\n",
    "    # get path of LIDC-IDRI directionary\n",
    "    data_dir = os.path.join(parent_dir, \"LIDC-IDRI\")\n",
    "    # give directory where docker saves files\n",
    "    docker_save_dir = r\"C:\\Users\\Diederik\\OneDrive\\Bureaublad\\studie tn\\Minor vakken Porto\\IA CAD\"\n",
    "    # give the hash of the pyradiomnics docker\n",
    "    docker_hash = r\"d95ce08239e3182d8631d3492a5e4a32096d28285c3d2f10dd570d7e6d06fd01\"\n",
    "    # path to the features dict\n",
    "    features_dict = r\"/data/test/featuresDict_IBSIv7.tsv\"\n",
    "    # pyradiomics save folder\n",
    "    pyradiomics_midsave_path = r\"/data/pyradiomics converter test\"\n",
    "    # temporal dir\n",
    "    temp_dir = r\"C:\\Users\\Diederik\\OneDrive\\Bureaublad\\studie tn\\Minor vakken Porto\\IA CAD\\test\\temp file\"\n",
    "    parameter_file = r\"C:\\Users\\Diederik\\OneDrive\\Bureaublad\\studie tn\\Minor vakken Porto\\IA CAD\\test\\Pyradiomics_Params_test.yaml\"\n",
    "    data = pd.read_csv(r\"C:\\Users\\Diederik\\OneDrive\\Bureaublad\\studie tn\\Minor vakken Porto\\IA CAD\\test\\features.csv\")\n",
    "    df = pd.read_excel(r\"C:\\Users\\Diederik\\OneDrive\\Bureaublad\\studie tn\\Minor vakken Porto\\IA CAD\\test\\nodule_counts_by_patient.xlsx\")\n",
    "    df = df.drop(df.columns[[4, 5]], axis=1)\n",
    "    df.columns = ['Patient_ID', 'Total_Nodule_Count', 'NodG3','NodL3']\n",
    "    dataframe = pd.DataFrame(\n",
    "        columns=['Patient_ID', 'Nodule', ' Annotation', 'Subtlety', 'InternalStructure', 'Calcification', 'Sphericity',\n",
    "                 'Margin', 'Lobulation', 'Spiculation', 'Texture', 'Malignancy'])\n",
    "    backup = 0\n",
    "    iteration_counter = 1\n",
    "    for p_id in df['Patient_ID']:\n",
    "\n",
    "        print(\"Patient \" + str(p_id) + \"Processing\")\n",
    "        if os.path.isdir(os.path.join(data_dir, str(p_id))) == False:\n",
    "            print(\"Patient \" + str(p_id) + \" not found\")\n",
    "            continue  # if the patient folder doesn't exist, skip it\n",
    "\n",
    "        scan = pl.query(pl.Scan).filter(pl.Scan.patient_id == p_id).first()\n",
    "        nods = scan.cluster_annotations()\n",
    "\n",
    "        # path to the patient folder\n",
    "        patient_dir = os.path.join(data_dir, str(p_id))\n",
    "        # path to dicom ct-scans of patient\n",
    "        patient_dicom_path = scan.get_path_to_dicom_files()\n",
    "\n",
    "        patient_folders = os.path.join(patient_dir, os.listdir(patient_dir)[0])\n",
    "        if len(os.listdir(patient_folders))==1:\n",
    "            shutil.rmtree(os.path.join(patient_dir, os.listdir(patient_dir)[0]))\n",
    "            patient_folders = os.path.join(patient_dir, os.listdir(patient_dir)[0])\n",
    "            print(\"wrong folder removed!\")\n",
    "\n",
    "        # listing all the folders from a patient\n",
    "        patient_seg_folders = os.listdir(patient_folders)\n",
    "        for folder in patient_seg_folders:\n",
    "            if \"evaluations\" in folder:\n",
    "                shutil.rmtree(os.path.join(patient_folders,folder))\n",
    "        patient_seg_folders = os.listdir(patient_folders)\n",
    "        # saving the dicom images folder path\n",
    "        # get all seg folders for nodules later\n",
    "\n",
    "\n",
    "        #if scan is None: # if the scan is not available we continue\n",
    "        #    continue\n",
    "\n",
    "        nod = 1\n",
    "        annot = 0\n",
    "        for nodule in nods:\n",
    "            for ann in nodule:\n",
    "                if annot >= len(patient_seg_folders):\n",
    "                    continue\n",
    "                backup += 1 #backupcounter\n",
    "\n",
    "                iteration_counter += 1\n",
    "                seg_folder = os.path.join(patient_folders, patient_seg_folders[annot+1])\n",
    "\n",
    "                # check how many files are in the segmentation folder\n",
    "                seg_files = os.listdir(seg_folder)\n",
    "                if len(seg_files) <= 0:\n",
    "                    # add a row with NaN values to the dataframe\n",
    "                    data.loc[len(data)] = [None] * len(data.columns)\n",
    "                # iterating over each segmentation file\n",
    "                for file in os.listdir(seg_folder):\n",
    "                    if file.endswith(\".dcm\"):\n",
    "                        seg_file_path = os.path.join(seg_folder, file)\n",
    "                        print(\"docker run -v \\\"\" + docker_save_dir + \":/data\\\" \" + docker_hash + \" --input-image-dir \\\"/data/\" + os.path.relpath(patient_dicom_path, docker_save_dir).replace(chr(92),\"/\") +  \"\\\" --input-seg-file \\\"/data/\" + os.path.relpath(seg_file_path, docker_save_dir).replace(chr(92),\"/\") + \"\\\" --output-dir \\\"\" + pyradiomics_midsave_path + \"\\\" --volume-reconstructor dcm2niix --features-dict \\\"/data/\" + os.path.relpath(features_dict, docker_save_dir).replace(chr(92),\"/\") + \"\\\" --temp-dir \\\"/data/\" + os.path.relpath(temp_dir, docker_save_dir).replace(chr(92),\"/\") + \"\\\" --correct-mask --parameters \\\"/data/\" + os.path.relpath(parameter_file, docker_save_dir).replace(chr(92),\"/\") + \"\\\"\")\n",
    "                        os.system(\"docker run -v \\\"\" + docker_save_dir + \":/data\\\" \" + docker_hash + \" --input-image-dir \\\"/data/\" + os.path.relpath(patient_dicom_path, docker_save_dir).replace(chr(92),\"/\") +  \"\\\" --input-seg-file \\\"/data/\" + os.path.relpath(seg_file_path, docker_save_dir).replace(chr(92),\"/\") + \"\\\" --output-dir \\\"\" + pyradiomics_midsave_path + \"\\\" --volume-reconstructor dcm2niix --features-dict \\\"/data/\" + os.path.relpath(features_dict, docker_save_dir).replace(chr(92),\"/\") + \"\\\" --temp-dir \\\"/data/\" + os.path.relpath(temp_dir, docker_save_dir).replace(chr(92),\"/\") + \"\\\" --correct-mask --parameters \\\"/data/\" + os.path.relpath(parameter_file, docker_save_dir).replace(chr(92),\"/\") + \"\\\"\")\n",
    "\n",
    "                        try:\n",
    "                            testdata = pd.read_csv(r\"C:\\Users\\Diederik\\OneDrive\\Bureaublad\\studie tn\\Minor vakken Porto\\IA CAD\\test\\temp file\\Features\\1.csv\")\n",
    "                            #print(testdata)\n",
    "                            # append data to features.csv\n",
    "                            #print(data.info())\n",
    "                            #data = data.append(testdata)\n",
    "                            data = pd.concat([data, testdata], ignore_index=True)\n",
    "                            #print(data)\n",
    "\n",
    "\n",
    "                        except:\n",
    "                            # append a row with NaN values to the dataframe\n",
    "                            data.loc[len(data)] = [None] * len(data.columns)\n",
    "                            thisdir = os.getcwd()\n",
    "                            os.chdir(parent_dir)\n",
    "                            # write to a log file the patient name, the seg folder name and the file name\n",
    "                            log = open(\"log.txt\", \"a\")\n",
    "                            log.write(\"Failed to extract features from: \" + os.getcwd() + \"\\n\")\n",
    "                            log.write(\"SEG File: \" + file + \"\\n\\n\")\n",
    "                            os.chdir(thisdir)\n",
    "                            continue\n",
    "                        # delete temp folder\n",
    "                        os.system(\"rmdir /s /q temp\")\n",
    "                        print(\"\\n\\n\")\n",
    "                    else:\n",
    "                        # also append a row with NaN values to the dataframe\n",
    "                        data.loc[len(data)] = [None] * len(data.columns)\n",
    "\n",
    "                # create feature vector\n",
    "                feature = list(ann.feature_vals())\n",
    "                feature.insert(0, annot)\n",
    "                feature.insert(0, nod)\n",
    "                feature.insert(0, p_id)\n",
    "                dataframe.loc[len(dataframe)] = feature\n",
    "\n",
    "                thisdir = os.getcwd()\n",
    "\n",
    "                # create a backup of the dataframes every 10 iterations (every 5 annotations)\n",
    "                if backup % 10 == 0:\n",
    "                    current_time = time.time()\n",
    "                    runtime = (current_time - start_time)/60\n",
    "                    print('Iteration: ' + str(iteration_counter) + '-----Backup create------------time:' + str(runtime))\n",
    "                    os.chdir(r\"C:\\Users\\Diederik\\OneDrive\\Bureaublad\\studie tn\\Minor vakken Porto\\IA CAD\\test\\Backups\")\n",
    "\n",
    "                    data.to_csv(\"pyradiomicsBackup.csv\", index=False)\n",
    "                    dataframe.to_csv(\"pylidcBackup.csv\", index=False)\n",
    "\n",
    "                    df1 = pd.read_csv(\"pylidcBackup.csv\")\n",
    "                    df2 = pd.read_csv(\"pyradiomicsBackup.csv\")\n",
    "\n",
    "                    df3 = pd.concat([df1, df2], axis=1)\n",
    "                    df3.to_csv(\"total_data_obliterationBackup.csv\", index=False)\n",
    "                os.chdir(thisdir)\n",
    "\n",
    "                annot += 1\n",
    "            nod += 1\n",
    "    os.chdir(parent_dir)\n",
    "\n",
    "    dataframe.to_csv(\"pylidc.csv\", index=False)\n",
    "    data.to_csv(\"pyradiomics.csv\", index=False)\n",
    "\n",
    "    df1 = pd.read_csv(\"pylidc.csv\")\n",
    "    df2 = pd.read_csv(\"pyradiomics.csv\")\n",
    "\n",
    "    # concatenate the columns from both dataframes\n",
    "    df3 = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "    df3.to_csv(\"total_data_obliteration.csv\", index=False)\n",
    "\n",
    "#get_features()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T12:28:30.409195500Z",
     "start_time": "2023-11-02T12:28:26.980238Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Code for feature selection:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#-loading data-> backup of 29/10/2023\n",
    "#pyradiomics data\n",
    "pr_data = pd.read_csv(\"../Unsorted/Data files/CSV DATA FILES/backup 29_10/pyradiomicsBackup.csv\")\n",
    "\n",
    "#pylidc data\n",
    "pl_data = pd.read_csv(\"../Unsorted/Data files/CSV DATA FILES/backup 29_10/pylidcBackup.csv\")\n",
    "\n",
    "#total obliteration data\n",
    "df = pd.read_csv(\"../Unsorted/Data files/CSV DATA FILES/backup 29_10/total_data_obliterationBackup.csv\")\n",
    "\n",
    "def drop_columns(df):\n",
    "    #function for ropping columns manually\n",
    "    columns_to_drop = np.arange(0,10)\n",
    "    # columns_to_drop2 = np.arange(12,30)\n",
    "    # columns_to_drop = np.append(columns_to_drop,  columns_to_drop2)\n",
    "    columns_to_drop = np.append(columns_to_drop,  [11,12,13])\n",
    "    df.drop(columns=df.columns[columns_to_drop],inplace=True)\n",
    "    return df\n",
    "def cleaning_data(df):\n",
    "    df = df.drop(df[df['Malignancy'] == 3].index)\n",
    "    # Remove rows with NaN values in the \"Malignancy\" column\n",
    "    df.dropna(subset=[\"Malignancy\"], inplace=True)\n",
    "    return df\n",
    "\n",
    "def normalize_data(df):\n",
    "    normalized_data = (df-df.min())/(df.max()-df.min())\n",
    "    return normalized_data\n",
    "\n",
    "def create_category_column(df):\n",
    "    # Create a new \"Category\" column based on the \"Malignancy\" values\n",
    "    df[\"Category\"] = df[\"Malignancy\"].apply(lambda x: 1 if x in [4, 5] else 0)\n",
    "    # drop all categories which are not numerical\n",
    "    df = df.select_dtypes(include=[int, float])\n",
    "    return df\n",
    "\n",
    "def get_malignancy_column_dtype(df):\n",
    "    # Check if the \"Malignancy\" column exists in the DataFrame\n",
    "    if \"Malignancy\" in df.columns:\n",
    "        malignancy_dtype = df[\"Malignancy\"].dtype\n",
    "        return malignancy_dtype\n",
    "    else:\n",
    "        return \"Column 'Malignancy' not found in the DataFrame.\"\n",
    "\n",
    "\n",
    "df = cleaning_data(df)\n",
    "get_malignancy_column_dtype(df)\n",
    "df = create_category_column(df)\n",
    "df_norm = normalize_data(df)\n",
    "#df_norm_form = drop_columns(df_norm)\n",
    "print(df[\"Malignancy\"])\n",
    "print(df[\"Category\"])\n",
    "print(df_norm)\n",
    "\n",
    "#getting rid of features with low variance\n",
    "#sel = VarianceThreshold(threshold=0)\n",
    "#sel.fit_transform(tot_data)\n",
    "#print(tot_data)\n",
    "\n",
    "df_norm.to_csv(r\"C:\\Users\\Diederik\\OneDrive\\Bureaublad\\studie tn\\Minor vakken Porto\\IA CAD\\test\\backup 29_10\\filewip.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-02T12:28:30.641191200Z",
     "start_time": "2023-11-02T12:28:30.431096600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T12:28:30.433108Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T12:28:30.433108Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T12:28:30.434571600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T12:28:30.440641300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T12:28:30.450164700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-02T12:28:30.454162100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LAB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
